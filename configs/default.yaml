name: default
data:
  data_dir: './data/download/files/'
  csv_out: './train/data.csv'
  remake_csv: True
  batch_size: 2
  shuffle: True
  img_height: 128
  dataset_split: [.8, .1, .1]  # train, val, test
  vocab_path: './train/vocab/vocab.txt'
  max_chord_stack: 10
model:
  img_channels: 1
  conv_blocks: 4
  conv_filter_n: [32, 64, 128, 256]
  conv_filter_size: [ [3,3], [3,3], [3,3], [3,3] ]
  conv_pooling_size: [ [2,2], [2,2], [2,2], [2,2] ]
  encoder_rnn_units: 512
  encoder_rnn_layers: 2
  decoder_hidden_size: 512
  dropout: TODO
training:
  random_seed: 42
  log_dir: './train/log/'
  lr: 1e-4
  max_epochs: 500
  print_training: False
  print_every: 50  # Batches
  decode_every: 50  # Batches
  save_every: 500  # Batches
